CONFIG = {
    "experiment": {
        "name": "rag_big_chunk_charsplit_metrics",
    },
    "llm_judge": {
        "provider": "OPENAI",
        "model_name": "gpt-5",
        "temperature": 1,
        "seed": 42,
    },
    "prompt": {
    "templateJudge": (
            'Ты проффесиональный оценщик ответов на вопросы по документу. '
            'Твоя задача сравнить получившийся ответ на вопрос с уже существующим ответом по смыслу опираясь на контекст, ты должен выставить ему оценку опираясь на систему оценивания '
            'В ответ пиши только цифру\n'
            'Cистема оценивания:\n0 - ответ не отвечает на вопрос, а существующий отвечает, хотя в контексте есть нужная информация\n'
            '1 - ответ не отвечает на вопрос, а существующий отвечает, и в контексте нет нужной информации\n'
            '2 - Ответ отвечает на вопрос или вопрос не связан с темой контекста и документа и модель отвечает что он не связан с документом \n3 - Ответ лучше существующего\n'
            "Контекст:\n{context}\n\n"
            "Вопрос:\n{question}\n\n"
            "Ответ:\n{answer}\n\n"
            "Существующий ответ:\n{answer_gpt}"

        )
    },
    "metrics": {
        "metrics_file": "judge_results.csv",
        "qwen_output_file": "qwen_gpt.csv",
        "metrics_folder": "./metrics/",
        "graphics_folder": "./graphics/"
    }
}